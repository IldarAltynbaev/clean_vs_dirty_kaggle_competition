{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15282,"databundleVersionId":565187,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import StratifiedKFold\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport zipfile\nwith zipfile.ZipFile('/kaggle/input/platesv2/plates.zip', 'r') as zip_obj:\n   # Extract all the contents of zip file in current directory\n   zip_obj.extractall('/kaggle/working/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"/kaggle/working/\"))\n\ndata_root = '/kaggle/working/plates/'\nprint(os.listdir(data_root))\n\nimport shutil \nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\noverall_dir = '/kaggle/working/overall_dataset'\nclass_names = ['cleaned', 'dirty']\n\nloop_number = 0\n\n\n\nos.makedirs(overall_dir, exist_ok=True)\n#for class_name in class_names:\n#    os.makedirs(os.path.join(overall_dir, class_name), exist_ok=True)\nimage_num = 0\n    \nlist_of_dirs = ['/kaggle/working/overall_dataset/cleaned','/kaggle/working/overall_dataset/dirty']\nlist_of_plates = []\nfor class_name in class_names:   \n    for file in os.listdir(data_root+'train/'+class_name):\n        file_name = os.fsdecode(file)\n        if (file.endswith(\".jpg\")):  \n            shutil.copy(os.path.join(data_root+'train/'+class_name, file_name), \n                        os.path.join(overall_dir, class_name + ' ' + str(image_num) + '.jpg'))\n            list_of_plates.append(class_name + ' ' + str(image_num) + '.jpg')\n            image_num = image_num + 1\n\nimport torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nimport random\nfrom PIL import Image\n\n\nfrom torchvision import transforms, models\nfrom torchvision.transforms import functional\nfrom torch.utils.data import ConcatDataset\nfrom matplotlib.animation import FuncAnimation\nbatch_size = 8\n\n\n#for each_dir in list_of_dirs:\n\n            \nlist_of_classes = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n\ndf = pd.DataFrame({'Model' : [],\n               'freezed' : [],\n               'augment' : [],\n                'num_of_aug' : [],\n               'Lr' : [],\n               'Ss' : [],\n               'Gm' : [],\n               'optim' : [],\\\n              'num_ep' : [],\n                'mean_loss' : [],\\\n                 'file_name' : [],\\\n                'loop_num' : [],\\\n              'submission_score' : []})\n\n\n\ndef set_random_seed(random_seed):\n    torch.manual_seed(random_seed)\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    \nset_random_seed(0)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nstrat_kfold = StratifiedKFold(n_splits=5, shuffle=True)\n\nfor fold, (train_ids, test_ids) in enumerate(strat_kfold.split(list_of_plates,list_of_classes)):\n    \n    fold_dir = '/kaggle/working/'+'fold ' + str(fold)\n    os.makedirs(fold_dir, exist_ok=True)\n    os.makedirs(fold_dir+'/train', exist_ok=True)\n    os.makedirs(fold_dir+'/train/cleaned', exist_ok=True)\n    os.makedirs(fold_dir+'/train/dirty', exist_ok=True)\n    os.makedirs(fold_dir+'/val', exist_ok=True)\n    os.makedirs(fold_dir+'/val/cleaned', exist_ok=True)\n    os.makedirs(fold_dir+'/val/dirty', exist_ok=True)\n    \n    for each_elem in train_ids:\n        \n        if (each_elem < 20):\n            shutil.copy(os.path.join(overall_dir, list_of_plates[each_elem]), \n                            os.path.join(fold_dir+'/train/cleaned', list_of_plates[each_elem]))\n        else:          \n            shutil.copy(os.path.join(overall_dir, list_of_plates[each_elem]), \n                os.path.join(fold_dir+'/train/dirty', list_of_plates[each_elem]))\n                        \n    for each_elem in test_ids:\n        \n        if each_elem < 20:\n            shutil.copy(os.path.join(overall_dir, list_of_plates[each_elem]), \n                            os.path.join(fold_dir+'/val/cleaned', list_of_plates[each_elem]))\n        else:          \n            shutil.copy(os.path.join(overall_dir, list_of_plates[each_elem]), \n                os.path.join(fold_dir+'/val/dirty', list_of_plates[each_elem]))\n            \ntest_dir = 'test'\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-23T07:45:25.281540Z","iopub.execute_input":"2024-09-23T07:45:25.281843Z","iopub.status.idle":"2024-09-23T07:45:33.615005Z","shell.execute_reply.started":"2024-09-23T07:45:25.281810Z","shell.execute_reply":"2024-09-23T07:45:33.613980Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['platesv2']\nAfter zip extraction:\n['.virtual_documents', 'plates', '__MACOSX']\n['test', 'train', '.DS_Store']\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'test/unknown'"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_data_set(train_dir,val_dir,batch_size,num_of_augmentaions):\n    \n    list_of_train_datasets = []        \n    \n    for i in range(num_of_augmentaions):\n          \n        train_transforms = create_train_transforms()\n        train_dataset = torchvision.datasets.ImageFolder(train_dir,train_transforms)   \n        \n        list_of_train_datasets.append(train_dataset)\n            \n    train_dataset = ConcatDataset(list_of_train_datasets)\n    \n    val_transforms = create_val_transforms()\n    val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n    \n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n    val_dataloader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)\n    \n    return train_dataloader, val_dataloader\n\nclass ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n\ndef show_transformed_image (transrorm, img):\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    \n    pic_tensor = transrorm(img)\n    pic_array = pic_tensor.permute(1, 2, 0).numpy()* std + mean\n    \n    rows = 2\n    columns = 2\n    fig = plt.figure(figsize=(10, 7)) \n    fig.add_subplot(rows, columns, 1) \n\n    # showing image \n    plt.imshow(img) \n    plt.axis('off') \n    plt.title(\"Before\") \n\n    # Adds a subplot at the 2nd position \n    fig.add_subplot(rows, columns, 2) \n\n    # showing image \n    plt.imshow(pic_array) \n    plt.axis('off') \n    plt.title(\"After\") \n        \ndef show_input(input_tensor, title=''):\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n    \ndef train_model(device,fold_num,model, loss,optimizer, \n                scheduler,accuracy_history_train,accuracy_history_val,\n                epoch_history,train_dataloader,val_dataloader,best_model, num_epochs):    \n    best_val_acc = 0\n    best_train_acc = 0\n    for epoch in tqdm(range(num_epochs)):\n        \n        epoch_history.append(epoch)\n        #print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                dataloader = val_dataloader\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # forward and backward\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss_value.item()\n                #print(preds_class)\n                #print(labels.data)\n                running_acc += (preds_class == labels.data).float().mean()\n\n            epoch_loss = running_loss / len(dataloader)\n            epoch_acc = running_acc / len(dataloader)\n            if phase == 'train':\n                accuracy_history_train.append(epoch_acc.item())\n            else:\n                if epoch_acc.item() >= best_val_acc:\n                    #print('best model - ' + str(epoch_acc.item()) + ' val accuracy')\n                    torch.save(model.state_dict(),'/kaggle/working/best_modelx_fold_'+ str(fold_num) +'.pt')\n                    best_val_acc = epoch_acc.item() \n                \n                    #print(model.state_dict())\n                    #best_model = model.state_dict()\n                accuracy_history_val.append(epoch_acc.item())\n\n            #print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model\n\ndef perform_training_instance(fold_num,\n                              batch_size,\n                              train_dir,\n                              val_dir,\n                              num_epochs,\n                              num_of_augmentaions,\n                              learning_rate,\n                              step_size,\n                              gamma,\n                              optimizer_name,\n                              model_name,\n                              model_freeze):\n    \n    model = create_blank_model(model_name, model_freeze)\n    loss = torch.nn.CrossEntropyLoss()    \n    optimizer = create_optimizer(model,learning_rate,optimizer_name)\n    scheduler = create_scheduler(optimizer,step_size,gamma)\n    train_dataloader,val_dataloader = prepare_data_set(train_dir,\n                                                       val_dir,batch_size,\n                                                       num_of_augmentaions)\n\n    epoch_history = []\n    accuracy_history_train = []\n    accuracy_history_val = []\n    best_model = 0\n    train_model(device,\n                fold_num,model,\n                loss, \n                optimizer, \n                scheduler, \n                accuracy_history_train,\n                accuracy_history_val,\n                epoch_history,\n                train_dataloader,\n                val_dataloader,\n                best_model,\n                num_epochs)\n\n    print(max(accuracy_history_train))\n    print(max(accuracy_history_val))\n    plt.figure(figsize=(20,12))\n    plt.plot(epoch_history ,accuracy_history_train,label = 'train')\n    plt.plot(epoch_history ,accuracy_history_val,label = 'val')\n    plt.legend()\n    plt.show()\n    \n\n    \ndef evaluate_model(model_path, val_dir, model_name,model_freeze):\n   \n    val_transforms = create_val_transforms()\n    val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n\n    val_dataloader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    \n    model = create_blank_model(model_name,model_freeze)\n    model.load_state_dict(torch.load(model_path))\n    model.to(device)\n    model.eval()\n\n\n    val_predictions = []\n    val_onehot = []\n    val_img_paths = []\n    for inputs, labels in val_dataloader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        with torch.set_grad_enabled(False):\n            preds = model(inputs)\n            val_onehot.append(preds.argmax(dim=1))\n            m = torch.nn.Softmax(dim=1)\n            #preds = m(preds)\n\n        val_predictions.append(m(preds))\n\n    loss_func = torch.nn.CrossEntropyLoss()\n    preds = preds.to(device)\n    tens = torch.Tensor([0,0,0,0,1,1,1,1]).to(device).type(torch.LongTensor)\n    tens = tens.to(device)\n    print(loss_func(preds,tens))\n\n    #print(val_predictions)\n    #print(val_onehot)\n    \ndef evaluate_ensemble(model_name,model_freeze):\n    \n    list_of_models = initiate_models(model_name,model_freeze)\n\n    list_of_valDirs = ['/kaggle/working/fold 0/val',\n                      '/kaggle/working/fold 1/val',\n                      '/kaggle/working/fold 2/val',\n                      '/kaggle/working/fold 3/val',\n                      '/kaggle/working/fold 4/val']\n    val_onehot = []\n    val_predictions = []\n    list_of_preds = []\n    list_of_loss = []\n    for index, each_dir in enumerate(list_of_valDirs):\n        \n        val_transforms = create_val_transforms()\n        val_dataset = torchvision.datasets.ImageFolder(each_dir, val_transforms)\n\n        val_dataloader = torch.utils.data.DataLoader(\n            val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n        \n        \n        \n        m = torch.nn.Softmax(dim=1)\n\n\n        for inputs, labels in val_dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            with torch.set_grad_enabled(False):\n                preds = list_of_models[index](inputs)\n                val_onehot.append(preds.argmax(dim=1))\n\n                list_of_preds.append(m(preds)) \n\n            val_predictions.append(m(preds))\n\n            loss_func = torch.nn.CrossEntropyLoss()\n            preds = preds.to(device)\n            tens = torch.Tensor([0,0,0,0,1,1,1,1]).to(device).type(torch.LongTensor)\n            tens = tens.to(device)\n            list_of_loss.append(loss_func(preds,tens))\n            #print(loss_func(preds,tens))\n\n    #print(list_of_preds)\n    #print(torch.stack(list_of_preds).mean(dim=0))\n    #print(val_onehot)\n    #print(list_of_loss)\n    mean_loss = torch.stack(list_of_loss).mean()\n    print(mean_loss)\n    \n    \n    return mean_loss\n    \ndef initiate_models(model_name,model_freeze):\n    \n    list_of_models = []\n    \n    model0 = create_blank_model(model_name,model_freeze)\n    model0.load_state_dict(torch.load('/kaggle/working/best_modelx_fold_0.pt'))\n    model0.to(device)\n    model0.eval()\n\n    list_of_models.append(model0)\n\n    model1 = create_blank_model(model_name,model_freeze)\n    model1.load_state_dict(torch.load('/kaggle/working/best_modelx_fold_1.pt'))\n    model1.to(device)\n    model1.eval()\n\n    list_of_models.append(model1)\n\n\n    model2 = create_blank_model(model_name,model_freeze)\n    model2.load_state_dict(torch.load('/kaggle/working/best_modelx_fold_2.pt'))\n    model2.to(device)\n    model2.eval()\n\n    list_of_models.append(model2)\n\n    model3 = create_blank_model(model_name,model_freeze)\n    model3.load_state_dict(torch.load('/kaggle/working/best_modelx_fold_3.pt'))\n    model3.to(device)\n    model3.eval()\n\n    list_of_models.append(model3)\n\n    model4 = create_blank_model(model_name,model_freeze)\n    model4.load_state_dict(torch.load('/kaggle/working/best_modelx_fold_4.pt'))\n    model4.to(device)\n    model4.eval()\n    \n    list_of_models.append(model4)\n    \n    return list_of_models\n\ndef model_enseble_predict(inputs,list_of_models):\n    \n    list_of_preds = []\n    \n    for each_model in list_of_models:\n        inputs.to(device)\n        each_model.to(device)\n        preds = each_model(inputs)\n        m = torch.nn.Softmax(dim=1)\n        preds = m(preds)\n        list_of_preds.append(preds)\n \n    return torch.stack(list_of_preds).mean(dim=0)\n\ndef create_blank_model(model_name, model_freeze):\n    assert model_name in ['resnet18','resnet34','resnet50']\n    assert model_freeze in ['except_last_layer','freeze_all_conv', 'unfreeze_last_layer4', \n                           'unfreeze_last_layer4_layer3','unfreeze_all']\n    \n    if model_name == 'resnet18':\n        \n        model = models.resnet18(pretrained=True)\n        \n    elif model_name == 'resnet34':\n        \n        model = models.resnet34(pretrained=True)\n            \n    elif model_name == 'resnet50':\n            \n        model = models.resnet50(pretrained=True)\n\n        # Disable grad for all conv layers\n    if model_freeze == 'except_last_layer':\n        for param in model.parameters():\n            #if isinstance(param, torch.nn.Conv2d):\n                param.requires_grad = False\n    elif model_freeze == 'freeze_all_conv':\n        for param in model.parameters():\n            if isinstance(param, torch.nn.Conv2d):\n                param.requires_grad = False\n                \n    elif model_freeze == 'unfreeze_last_layer4':\n        for param in model.parameters():\n            param.requires_grad = False\n            \n        for param in model.layer4[0].parameters():  \n            param.requires_grad = True\n            \n    elif model_freeze == 'unfreeze_last_layer4_layer3':\n        for param in model.parameters():\n            param.requires_grad = False\n            \n        for param in model.layer4[0].parameters():  \n            param.requires_grad = True      \n            \n        for param in model.layer3[0].parameters():  \n            param.requires_grad = True  \n            \n    elif model_freeze == 'unfreeze_all':\n        for param in model.parameters():\n            param.requires_grad = True\n    \n       \n\n    model.fc = torch.nn.Linear(model.fc.in_features, 2)    \n    model = model.to(device)\n    return model\n\ndef create_optimizer(model,learning_rate,optimizer_name):\n    assert optimizer_name in ['adam', 'sgd']\n    if optimizer_name == 'adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n    elif optimizer_name == 'sgd':\n        optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n    return optimizer\n    \ndef create_scheduler(optimizer,step_size,gamma):\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n    return optimizer\n\n\ndef create_train_transforms():\n\n    train_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop((60, 60)),\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])   \n    return train_transforms\n\ndef add_results_to_dataframe (df,model_name, \n                             model_freeze,\n                             tr_transforms,\n                             num_of_augmentaions,\n                             learning_rate,\n                             step_size,\n                             gamma,\n                             optimizer_name,\n                             num_of_epochs,\n                             mean_loss,\n                             file_name,\n                             loop_number):\n    \n    \n\n    \n    df_new_row = pd.DataFrame({'Model' : [model_name],\n               'freezed' : [model_freeze],\n               'augment' : [str(tr_transforms)],\n                'num_of_aug' : [num_of_augmentaions],\n               'Lr' : [learning_rate],\n               'Ss' : [step_size],\n               'Gm' : [gamma],\n               'optim' : [optimizer_name],\\\n              'num_ep' : [num_of_epochs],\n                'mean_loss' : [mean_loss.item()],\\\n                 'file_name' : [file_name],\\\n                'loop_num' : [loop_number],\\\n              'submission_score' : [None]})\n    \n\n  \n    \n    return pd.concat([df, df_new_row], ignore_index = True)\n   \n\ndef create_data_submission_file(model_name,model_freeze,loop_number):\n    list_of_models = initiate_models(model_name,model_freeze)\n    \n    val_transforms = create_val_transforms()\n    test_dataset = ImageFolderWithPaths('/kaggle/working/test', val_transforms)\n\n    test_dataloader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\n\n    test_predictions = []\n    test_img_paths = []\n    for inputs, labels, paths in tqdm(test_dataloader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        with torch.set_grad_enabled(False):\n            preds = model_enseble_predict(inputs,list_of_models)\n        test_predictions.append(\n            torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n        test_img_paths.extend(paths)\n\n\n    test_predictions = np.concatenate(test_predictions)\n\n    submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})\n    submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned')\n    submission_df['id'] = submission_df['id'].str.replace('/kaggle/working/test/unknown/', '')\n    submission_df['id'] = submission_df['id'].str.replace('.jpg', '')\n    submission_df.set_index('id', inplace=True)\n    submission_df.head(n=6)\n\n    file_name = \"submission_resnet\" + str(loop_number) + \".csv\"\n    submission_df.to_csv(file_name)\n    \n    return file_name\n    \ndef kfold_training(batch_size,\n                   num_of_epochs,\n                   num_of_augmentaions,\n                   learning_rate,\n                   step_size,\n                   gamma,\n                   optimizer_name,\n                   model_name,\n                   model_freeze,\n                   loop_number,\n                   df):\n    perform_training_instance(0,batch_size,'/kaggle/working/fold 0/train','/kaggle/working/fold 0/val',\\\n                               num_of_epochs,\n                             num_of_augmentaions,learning_rate,step_size,gamma,\n                   optimizer_name,\n                   model_name,model_freeze)\n    perform_training_instance(1,batch_size,'/kaggle/working/fold 1/train','/kaggle/working/fold 1/val',\\\n                                num_of_epochs,\n                             num_of_augmentaions,learning_rate,step_size,gamma,\n                   optimizer_name,\n                   model_name,model_freeze)\n    perform_training_instance(2,batch_size,'/kaggle/working/fold 2/train','/kaggle/working/fold 2/val',\\\n                               num_of_epochs,\n                             num_of_augmentaions,learning_rate,step_size,gamma,\n                   optimizer_name,\n                   model_name,model_freeze)\n    perform_training_instance(3,batch_size,'/kaggle/working/fold 3/train','/kaggle/working/fold 3/val',\\\n                                num_of_epochs,\n                             num_of_augmentaions,learning_rate,step_size,gamma,\n                   optimizer_name,\n                   model_name,\n                             model_freeze)\n    perform_training_instance(4,batch_size,'/kaggle/working/fold 4/train','/kaggle/working/fold 4/val',\\\n                                num_of_epochs,\n                             num_of_augmentaions,learning_rate,step_size,gamma,\n                   optimizer_name,\n                   model_name,model_freeze)\n    \n    evaluate_model('/kaggle/working/best_modelx_fold_0.pt', '/kaggle/working/fold 0/val'\\\n                   ,model_name,model_freeze)\n    evaluate_model('/kaggle/working/best_modelx_fold_1.pt', '/kaggle/working/fold 1/val'\\\n                   ,model_name,model_freeze)\n    evaluate_model('/kaggle/working/best_modelx_fold_2.pt', '/kaggle/working/fold 2/val'\\\n                   ,model_name,model_freeze)\n    evaluate_model('/kaggle/working/best_modelx_fold_3.pt', '/kaggle/working/fold 3/val'\\\n                   ,model_name,model_freeze)\n    evaluate_model('/kaggle/working/best_modelx_fold_4.pt', '/kaggle/working/fold 4/val'\\\n                   ,model_name,model_freeze)\n    \n    mean_loss = evaluate_ensemble(model_name,model_freeze)\n    \n   \n    file_name = create_data_submission_file(model_name,model_freeze,loop_number)\n    \n    tr_transforms = create_train_transforms()\n    result_df = add_results_to_dataframe(df,model_name, \n                             model_freeze,\n                             tr_transforms,\n                             num_of_augmentaions,\n                             learning_rate,\n                             step_size,\n                             gamma,\n                             optimizer_name,\n                             num_of_epochs,\n                             mean_loss,\n                             file_name,\n                             loop_number)\n    df = result_df\n    loop_number = loop_number+1\n    return df, loop_number","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:46:26.258404Z","iopub.execute_input":"2024-09-23T07:46:26.259093Z","iopub.status.idle":"2024-09-23T07:46:26.334039Z","shell.execute_reply.started":"2024-09-23T07:46:26.259052Z","shell.execute_reply":"2024-09-23T07:46:26.333041Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def create_train_transforms():\n\n    train_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop((60, 60)),\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])   \n    return train_transforms\n\ndef create_val_transforms():\n    val_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop((60, 60)),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    return val_transforms\n\n\nlearning_rate =1.0e-3\nstep_size=7\ngamma=0.1\nmodel_name = 'resnet34'\nmodel_freeze = 'freeze_all_conv'\nnum_of_epochs = 60\nnum_of_augmentaions = 20\noptimizer_name = 'sgd'\n\ndf, loop_number = kfold_training(batch_size,\n                   num_of_epochs,\n                   num_of_augmentaions,\n                   learning_rate,\n                   step_size,\n                   gamma,\n                   optimizer_name,\n                   model_name,\n                   model_freeze,\n                   loop_number,\n                   df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_transforms():\n\n    train_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop((75, 75)),\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])   \n    return train_transforms\n\ndef create_val_transforms():\n    val_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop((75, 75)),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    return val_transforms\n\n\nlearning_rate =1.0e-3\nstep_size=7\ngamma=0.1\nmodel_name = 'resnet34'\nmodel_freeze = 'freeze_all_conv'\nnum_of_epochs = 60\nnum_of_augmentaions = 20\noptimizer_name = 'sgd'\n\ndf, loop_number = kfold_training(batch_size,\n                   num_of_epochs,\n                   num_of_augmentaions,\n                   learning_rate,\n                   step_size,\n                   gamma,\n                   optimizer_name,\n                   model_name,\n                   model_freeze,\n                   loop_number,\n                   df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_transforms():\n\n    train_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop((90, 90)),\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])   \n    return train_transforms\n\ndef create_val_transforms():\n    val_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop((90, 90)),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    return val_transforms\n\n\nlearning_rate =1.0e-3\nstep_size=7\ngamma=0.1\nmodel_name = 'resnet34'\nmodel_freeze = 'freeze_all_conv'\nnum_of_epochs = 60\nnum_of_augmentaions = 20\noptimizer_name = 'sgd'\n\ndf, loop_number = kfold_training(batch_size,\n                   num_of_epochs,\n                   num_of_augmentaions,\n                   learning_rate,\n                   step_size,\n                   gamma,\n                   optimizer_name,\n                   model_name,\n                   model_freeze,\n                   loop_number,\n                   df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('intermediate_result.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}